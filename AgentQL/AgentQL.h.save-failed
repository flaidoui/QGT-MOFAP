#ifndef AGENTQL_H
#define AGENTQL_H

#include <string>
#include <vector>
#include <map>
#include <set>
<<<<<<< HEAD

//====================================
#include "../Individual/Individual.h"

//=====================================
using namespace std;
const float GAMMA = 0.6;
const float ALPHA = 0.8;
//-----------------------------------------
//strategy frequency
//freqstat=1 Random, reqnum2 Cost .freqnum4 Nash .freqnum 6 Hypervolume .Dominance rank . 550

//---------------------------------
/*struct ActionPI {
    int TRXStratPI;
    int FreqStratPI;
};
struct ActionPS {
    int TRXStratPS;
    int FreqStratPS;
};
//---------------------------------------
*/
// on définit l'action comme étant le coupe (stratégie trx, strategie fréquence)
struct Action
{
    int ActTRX;
    int ActFreq;
};
//--------------------------------
struct State
{
    Individual * CurrentSol;
   // TRXs CurrentTRX;
};
//-------------------------------------------
class AGENTQL {

	public:
    AGENTQL(int IndexPlayer);//IndexPlayer = 1 si PI est est entrain de joueur ;IndexPlayer = ; si PS est entrain de joueur;
    //IndexPlayer = -1; si aucun joeueur n'est entrain de joueur
    ~AGENTQL();
    //------------------------------------------------------------------------------------------------------------------
    //--------------------------------------------------------------------------------------------------------------
   // vector<int,int> StatesSet; //vecteurs etats : solutions+strategies (TRX,Freq) PI + strategy (TRX,Freq)PS
    vector < Action > * ActionsSet; //vecteurs actions; prédéfinies à l'avance
    double QGAMMA;//combiengt on prète importance des informations à long terme; glment = 0.9
    double QALPHA;//glment =0.01
    //-------------------------------------------------------------------------------------------------------------
    //------------------------------------------------------------------------------------------------------------
    map < pair < State ,Action>,double > * Qtable; //matrice (state, action)== (currentsol, action effectuée)== qvalue (valeur hv calculée)  //
   // vector < map < pair < State, Action> , double > > RewardTable;
    void InitQtable(AFP * afpProblem, const int max_gen);
    Action GetPossibleAction (State s);// recupérer l'action
    double GetMaxQValue(State s, Action & CorrespondingAction);//récupérer la valeur max de Qtable et retourner l'action y associé  dans CorrespondingAction
    void UpdateQtable (State s, Action Act);//mise à jour de la table Qtable

};
#endif
=======

//====================================
#include "../Individual/Individual.h"

//=====================================
using namespace std;
const float GAMMA = 0.6;
const float ALPHA = 0.8;
//-----------------------------------------
//strategy frequency
//freqstat=1 Random, reqnum2 Cost .freqnum4 Nash .freqnum 6 Hypervolume .Dominance rank . 550

//---------------------------------
/*struct ActionPI {
    int TRXStratPI;
    int FreqStratPI;
};
struct ActionPS {
    int TRXStratPS;
    int FreqStratPS;
};
//---------------------------------------
*/
// on définit l'action comme étant le coupe (stratégie trx, strategie fréquence)
struct Action
{
    int ActTRX;
    int ActFreq;
};
//--------------------------------
struct State
{
    Individual * CurrentSol;
   // TRXs CurrentTRX;
};
//-------------------------------------------
class AGENTQL {

	public:
    AGENTQL(int IndexPlayer);//IndexPlayer = 1 si PI est est entrain de joueur ;IndexPlayer = ; si PS est entrain de joueur;
    //IndexPlayer = -1; si aucun joeueur n'est entrain de joueur
    ~AGENTQL();
    //------------------------------------------------------------------------------------------------------------------
    //--------------------------------------------------------------------------------------------------------------
   // vector<int,int> StatesSet; //vecteurs etats : solutions+strategies (TRX,Freq) PI + strategy (TRX,Freq)PS
    vector < Action > * ActionsSet; //vecteurs actions; prédéfinies à l'avance
    double QGAMMA;//combiengt on prète importance des informations à long terme; glment = 0.9
    double QALPHA;//glment =0.01
    //-------------------------------------------------------------------------------------------------------------
    //------------------------------------------------------------------------------------------------------------
    map < pair < State ,Action>,double > * Qtable; //matrice (state, action)== (currentsol, action effectuée)== qvalue (valeur hv calculée)  //
   // vector < map < pair < State, Action> , double > > RewardTable;
    void InitQtable(AFP * afpProblem, const int max_gen);
    Action GetPossibleAction (State s);// recupérer l'action
    double GetMaxQValue(State s, Action & CorrespondingAction);//récupérer la valeur max de Qtable et retourner l'action y associé  dans CorrespondingAction
    void UpdateQtable (State s, Action Act);//mise à jour de la table Qtable

};
#endif
>>>>>>> 0514ea9fae0f841576757defb02235a9189b0fad
